### 5장. 데이터 분석
- 엘라스틱서치로 도큐먼트 텍스트 분석하기
- 분석 API 사용하기
- 토큰화
- 문자 필터
- 토큰 필터
- 스태밍
- 엘라스틱서치에 포함된 분석기 

#### 5.1 무엇을 분석할 것인가?
1. 문자 필터링 - 문자 필터를 이용해 문자들을 변환
2. 텍스트를 토큰으로 분해 - 텍스트를 한 개 이상의 토큰 집합으로 분해
3. 토큰 필터링 - 토큰 필터를 사용하여 개별 토큰을 변환
4. 토큰 색인 - 토큰을 색인에 저장함 

###### 5.1.1 문자 필터링
- 먼저 문자 필터를 실행함
- 이들 필터는 연속된 특정 문자를 다른 문자로 변환하는 데 사용함 

###### 5.1.2 토큰으로 분해하기
- 텍스트에 문자 필터가 적용된 후 조각으로 분해함 
- 토큰이 몇 개가 되든 텍스트 조각으로부터 생성함 

###### 5.1.3 토큰 필터링 
- 토큰 필터로 불리는 것을 개별 토큰에 적용함

###### 5.1.4 토큰 색인
- 0개 또는 그 이상의 토큰 필터를 거친 토큰은 색인을 위해 루씬으로 전송됨 
- 이들 토큰은 역색인으로 만듦

#### 5.2 도큐먼트에 분석기 사용하기
- 자주 변경되지 않는 색인에서 같은 분석기 집합을 사용할 수 있음 
- 나중에 "put Mapping API"를 사용하여 색인의 매핑에서 어떤 필드가 어떤 분석기를 사용할지 지정할 수 있음

###### 5.2.1 색인 생성 시 분석기 추가하기
- 사용자 지정 분석기 -> 사용자 지정 토크나이저, 사용자 지정 필터, 사용자 지정 문자 필터 

###### 5.2.2 엘라스틱서치 설정으로 분석기 추가하기 
- 엘라스틱서치 설정에 분석기를 지정하면 분석기의 어떠한 변경 사항이라도 엘라스틱서치를 재시작해야 함 
- 반면, 색인을 생성할 때 전송하는 데이터는 적을 것 

###### 5.2.3 매핑에서 필드를 위한 분석기 지정하기
- 분석된 테스트를 다르게 저장하는 다중 필드 유형

#### 5.3 분석 API로 텍스트 분석하기
- 분석 API 사용: 분석 절차를 테스트
- 분석기, 토크나이저 또는 토큰 필터를 지정하여 엘라스틱서치에 어떠한 텍스트라도 보내 분석된 토큰을 돌려받게 함 

###### 5.3.1 분석기 선택하기
- 분석기의 이름을 analyzer 파라미터로 설정하기

###### 5.3.2. 즉흥적 분석기를 생성하는 부품 조합하기
- 분석 API로 텍스트를 분석하기 위해 사용하는 토크나이저와 토큰 필터 목록을 지정할 수 있음 

###### 5.3.3 필드 매핑에 기반한 분석 
###### 5.3.4 텀 백터 API를 사용하여 색인된 텀에 관해 배워보기
-_termvector 종단점을 사용해 텀이 도큐먼트와 색인에 얼마나 있는지 확인 가능 

#### 5.4 분석기, 토크나이저, 토큰 핉러
- 내장 분석기, 토크나이저, 토큰 필터 

###### 5.4.1 내장 분석기
- 분석기는 단일 토크나이저, 0 또는 그 이상의 토큰 필터와 함께 선택적인 문자 필터로 구성됨 

- 표준 분석기: 분석기를 명시하지 않을 때, 텍스트를 위한 기본 분석기
  - 필터를 위한 분석기를 명시하지 않으면 표준 분석기가 사용됨
- 단순 분석기: 소문자 토크나이저 사용
- 화이트스페이스: 텍스트를 화이트스페이스로 분해해서 토큰으로 만들기
- 불용어(stop): 토큰 스트림으로부터 불용어를 거름
- 키워드(keyword): 필드 전체를 단일 토큰으로 만듦, 색인 설정에 not_analyzed로 설정하는 것이 더 나음
- 패턴(pattern): 분해된 부분이 될 토큰의 패턴을 명시하도록 함 
- 언어 및 다국어: 특정 언어에 특화된 분석기를 명시함 
- 스노우볼: 스노우볼 스태머를 사용해서 텍스트로부터 어간을 추출함 

###### 5.4.2 토큰화
- 토큰화: 텍스트의 문자열을 가져와서 토큰이라는 더 작은 청크로 분해하는 것 
- 표준 토크나이저(standard tokenizer): 대부분의 유럽권 언어에 적합한 문법 기반 토크나이저 
- 키워드 토크나이저: 전체 텍스트를 가져와서 단일 토큰처럼 토큰 필터에 제공 
- 문자 토크나이저: 문자가 아닌 것으로 토큰을 나눔
- 소문자화: 정규 문자 토크나이저의 역할과 더불어 소문자화 토큰 필터를 결합함
- 화이트스페이스: 공백, 탭, 개행 등의 화이트스페이스로 토큰을 분해함 
- 패턴: 임의의 패턴을 명시하여 텍스트를 토큰으로 분해함 
- 경로 계층: 같은 경로를 분할하는 파일을 검색할 시스템 경로 색인 

###### 5.4.3 토큰 필터
- 표준 토큰 필터: 색인을 위해 토크나이저와 준비한 데이터로부터 토큰을 받아들임
- 소문자화 필터: 통과하는 토큰을 소문자로 만듦
- 길이 필터: 토큰의 min, max 길이의 경계 밖으로 떨어진 단어를 제거함
- 불용어 토큰 필터: 토큰 스트림으로부터 불용어 제거 
- truncate: 사용자 지정 설정에서 length 파라미터로 특정 길이가 넘어가는 토큰을 잘라냄
- trim: 토큰을 둘러싼 모든 화이트스페이스 제거
- limit token count: 특정 필드가 담을 수 있는 토큰의 최대 갯수 제한함 
- 반전 토큰 색인: 토큰 스트림으로부터 개별 토큰을 정반대로 뒤집음 
- 유일성 토큰 필터: 오직 유일한 토큰만 유지함 
- ascii folding: 일반 ASCII 문자에 해당하지 않는 유니코드 문자가 있따면, 이에 대응하는 ASCII로 변환함 
- 동의어 토큰 필터: 토큰 스트림의 단어를 원래 토큰과 동일한 오프셋 내에서 동의어로 교체함

#### 5.5 Ngram, edge ngram, shingle
- Ngram: 토큰의 각 단어 부분을 다중 서브 토큰으로 분해하는 방식

###### 5.5.1 1-gram 
- spaghetti -> s, p, a, g, h, e, t, t, i
###### 5.5.1 Bigram
- sp, pa, ag, gh, he, et, tt
###### 5.5.3 Trigram
- spa, pag...
###### 5.5.4 min_gram과 max_gram 설정
- ngram을 사용하여 철자가 틀린 단어도 찾을 수 있음
- 사전에 어떤 언어로 된 텍스트인지 모르거나 다른 유럽권언어가 서로 다른 방식으로 결합된 단어로 된 언어를 갖고 있을 때에도 텍스트 분석 가능
###### 5.5.5 Edge ngram
- 오직 앞부분부터 ngram을 만듬
###### 5.5.6 Ngram 설정
- Ngram은 단어 사이의 공백이 없어도 언어를 분석할 수 있음 
###### 5.5.7 Shingle
- 기본적으로 문자 수준이 아닌 토큰 수준에서 ngram 
- foo, foo bar...
- shingle 필터는 기본적으로 원래 토큰을 포함함 

#### 5.6 스태밍
- 스태밍: 단어를 단어의 원형이나 어근으로 줄이는 역할을 함 
- 단어의 어근이나 어간을 공유하는 단어 같은 것도 일치할 수 있음 

###### 5.6.1 스태밍 알고리즘
- 공식을 사용하거나 어간으로 만들기 위한 개별 토큰 규칙의 집합을 적용함 
- snowball, porter_stem, kstem 
###### 5.6.2 사전으로 스태밍 
- 단어를 어간으로 만드는 더 정확한 방식은 단어 사정을 사용하는 것 
- hunspell 토큰 필터 
###### 5.6.3 토큰 필터로부터 스태밍 오버라이드

#### 5.7 요약 
- 분석은 도큐먼트의 필드에 있는 텍스트를 토큰으로 만드는 과정 
- 매핑을 통해 개별 필드에 분석기를 할당함 
- 분석기는 하나 이상의 문자 필터가 하나 이상의 토큰 필터가 되는 토크나이저에 의해 만들어지는 체인을 처리함
- 문자 필터는 토크나이저로 보내기 전에 문자열을 처리하는데 사용됨
- 토크나이저는 문자열을 다수 개 토큰으로 분해하는 데 사용함 
- 토큰 필터는 토크나이저로부터 오는 토큰을 처리하는 데 사용함 
- Ngram 토큰 필터는 단어 일부로 토큰을 만듬
- Edge ngram은 오직 단어의 시작 또는 마지막으로부터 처리함